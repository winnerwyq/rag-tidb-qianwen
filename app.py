# -------------------- å®Œæ•´ä»£ç ï¼ˆå·²åˆå¹¶ä¿®å¤ï¼‰ --------------------
import streamlit as st
import pymupdf
import pymysql
import numpy as np
import hashlib
import json
from datetime import datetime
from typing import List, Dict, Any
from dashscope import Generation, TextEmbedding


# ---------------- é¡µé¢é…ç½® ----------------
st.set_page_config(
    page_title="TiDB + åƒé—® RAG Document Q&A",
    page_icon="ğŸ“š",
    layout="wide"
)

# ---------------- session state ----------------
for k in ['documents_count', 'last_query_time', 'db_initialized', 'rag_system']:
    if k not in st.session_state:
        st.session_state[k] = 0 if k == 'documents_count' else None if k == 'last_query_time' else False

# ---------------- RAG ç³»ç»Ÿ ----------------
class RAGSystem:
    def __init__(self):
        self.db_connection = None
        self.embedding_model = "text-embedding-v1"
        self.chat_model    = "qwen-max"
        self.chunk_size    = 1000
        self.chunk_overlap = 200
        self._inited       = False

    # ---------- åˆå§‹åŒ– ----------
    def initialize(self, api_key: str,
                   host: str, port: int, user: str, password: str, database: str) -> bool:
        import dashscope
        dashscope.api_key = api_key.strip()

        # åƒé—®æ¢æ´»
        try:
            Generation.call(model="qwen-turbo",
                            messages=[{"role": "user", "content": "ping"}],
                            max_tokens=1)
            self._inited = True
        except Exception as e:
            st.error(f"åƒé—®åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
            return False

        # è¿æ¥ TiDB
        try:
            self.db_connection = pymysql.connect(
                host=host, port=port, user=user, password=password,
                database=database, autocommit=True, ssl={"ssl": True}
            )
            self._init_db()
            return True
        except Exception as e:
            st.error(f"TiDB è¿æ¥å¤±è´¥ï¼š{e}")
            return False

    # ---------- å»ºè¡¨ ----------
    def _init_db(self):
        with self.db_connection.cursor() as cur:
            cur.execute("""
            CREATE TABLE IF NOT EXISTS qwen_documents (
                id INT AUTO_INCREMENT PRIMARY KEY,
                filename VARCHAR(255) NOT NULL,
                chunk_index INT NOT NULL,
                content TEXT NOT NULL,
                embedding VECTOR(1536) NOT NULL,
                file_hash VARCHAR(64) NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                INDEX idx_filename (filename),
                INDEX idx_file_hash (file_hash),
                VECTOR INDEX idx_embedding ((VEC_COSINE_DISTANCE(embedding)))
            )
            """)

    # ---------- æ–‡æœ¬æå– ----------
    def extract_pdf(self, file) -> str:
        doc = pymupdf.open(stream=file.read(), filetype="pdf")
        text = "".join(page.get_text() for page in doc)
        doc.close()
        return text

    def extract_txt(self, file) -> str:
        file.seek(0)
        return file.read().decode("utf-8")

    # ---------- åˆ†å— ----------
    def chunk_text(self, text: str) -> List[str]:
        words = text.split()
        step  = max(1, self.chunk_size - self.chunk_overlap)
        return [" ".join(words[i:i+self.chunk_size]) for i in range(0, len(words), step) if words[i:i+self.chunk_size]]

    # ---------- å‘é‡åŒ– ----------
    def embed(self, text: str) -> List[float]:
        resp = TextEmbedding.call(model=self.embedding_model, input=text)
        return [float(x) for x in resp.output["embeddings"][0]["embedding"]]

    # ---------- å­˜å‚¨ ----------
    def store(self, filename: str, chunks: List[str], file_hash: str):
        with self.db_connection.cursor() as cur:
            cur.execute("DELETE FROM qwen_documents WHERE filename=%s", (filename,))
            sql = "INSERT INTO qwen_documents (filename,chunk_index,content,embedding,file_hash) VALUES (%s,%s,%s,%s,%s)"
            for idx, chunk in enumerate(chunks):
                vec = self.embed(chunk)
                vec_str = "[" + ",".join(f"{x:.6f}" for x in vec) + "]"
                cur.execute(sql, (filename, idx, chunk, vec_str, file_hash))

    # ---------- æ£€ç´¢ ----------
    def search(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        vec = self.embed(query)
        vec_str = "[" + ",".join(f"{x:.6f}" for x in vec) + "]"
        with self.db_connection.cursor() as cur:
            cur.execute("""
            SELECT id, filename, content,
                   VEC_COSINE_DISTANCE(embedding, %s) AS similarity
            FROM qwen_documents
            ORDER BY similarity ASC
            LIMIT %s
            """, (vec_str, top_k))
            return [{"id": r[0], "filename": r[1], "content": r[2], "similarity": float(r[3])} for r in cur.fetchall()]

    # ---------- ç”Ÿæˆå›ç­” ----------
    def answer(self, query: str, chunks: List[Dict[str, Any]]) -> str:
        context = "\n\n".join(c["content"] for c in chunks)
        prompt = f"""Answer the question using only the provided context.
If the answer is not in the context, say you don't know.

Context:
{context}

Question: {query}

Answer:"""
        resp = Generation.call(
            model=self.chat_model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
            max_tokens=800,
            result_format="message"
        )
        return resp.output.choices[0]["message"]["content"].strip()

    # ---------- ç»Ÿè®¡ ----------
    def doc_count(self) -> int:
        with self.db_connection.cursor() as cur:
            cur.execute("SELECT COUNT(DISTINCT filename) FROM qwen_documents")
            return cur.fetchone()[0]

# ---------------- Streamlit ä¸»ç•Œé¢ ----------------
def main():
    st.title("TiDB + åƒé—® RAG Document Q&A")
    st.markdown("ä¸Šä¼ æ–‡æ¡£ï¼ŒåŸºäº **é˜¿é‡Œé€šä¹‰åƒé—®** ä¸ **TiDB Vector** è¿›è¡Œé—®ç­”ã€‚")

    # åˆå§‹åŒ–
    if not st.session_state.db_initialized:
        with st.spinner("ğŸš€ æ­£åœ¨åˆå§‹åŒ–â€¦â€¦"):
            sec = st.secrets
            rag = RAGSystem()
            ok = rag.initialize(sec["DASHSCOPE"]["DASHSCOPE_API_KEY"],
                                sec["TIDB"]["TIDB_HOST"],
                                int(sec["TIDB"]["TIDB_PORT"]),
                                sec["TIDB"]["TIDB_USER"],
                                sec["TIDB"]["TIDB_PASSWORD"],
                                sec["TIDB"]["TIDB_DATABASE"])
            if ok:
                st.session_state.rag_system = rag
                st.session_state.db_initialized = True
                st.session_state.documents_count = rag.doc_count()
                st.success("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
            else:
                st.error("âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ secrets.toml")
                st.stop()

    rag = st.session_state.rag_system

    # ç»Ÿè®¡å¡ç‰‡
    col1, col2, col3 = st.columns(3)
    with col1: st.metric("ğŸ“„ æ–‡æ¡£æ•°", st.session_state.documents_count)
    with col2: st.metric("ğŸ”§ çŠ¶æ€", "å°±ç»ª")
    with col3: st.metric("â° ä¸Šæ¬¡æŸ¥è¯¢", st.session_state.last_query_time.strftime("%H:%M:%S") if st.session_state.last_query_time else "æ— ")

    st.divider()

    col_left, col_right = st.columns([1, 1])

    # å·¦ä¾§ï¼šä¸Šä¼ 
    with col_left:
        st.header("ä¸Šä¼ æ–‡æ¡£")
        # â‘  åŠ  keyï¼Œæ–¹ä¾¿åé¢æ¸…æ‰
        file = st.file_uploader("é€‰æ‹© PDF / TXT æ–‡ä»¶", type=["pdf", "txt"], key="uploader")
    
        if file:
            with st.spinner("å¤„ç†ä¸­â€¦â€¦"):
                text = rag.extract_pdf(file) if file.type == "application/pdf" else rag.extract_txt(file)
                if not text.strip():
                    st.error("âŒ æå–æ–‡æœ¬å¤±è´¥")
                    st.stop()
    
                h = hashlib.sha256(text.encode()).hexdigest()
    
                with rag.db_connection.cursor() as cur:
                    cur.execute("SELECT 1 FROM qwen_documents WHERE file_hash=%s LIMIT 1", (h,))
                    if cur.fetchone():
                        st.warning("âš ï¸ è¯¥æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡")
                    else:
                        chunks = rag.chunk_text(text)
                        rag.store(file.name, chunks, h)
                        st.success(f"âœ… å·²å­˜å‚¨ {file.name}")
                        st.session_state.documents_count = rag.doc_count()
    
                    # â‘¡ å…³é”®ï¼šç«‹å³æ¸…ç©ºä¸Šä¼ æ§ä»¶
                    st.session_state.pop("uploader", None)


    # å³ä¾§ï¼šæé—®
    with col_right:
        st.header("æé—®")
        q = st.text_area("è¾“å…¥é—®é¢˜ï¼š", height=100)
        if st.button("ğŸ” æé—®", type="primary", use_container_width=True):
            if q:
                with st.spinner("æ£€ç´¢ä¸­â€¦â€¦"):
                    chunks = rag.search(q, top_k=5)
                    if chunks:
                        with st.spinner("ç”Ÿæˆå›ç­”â€¦â€¦"):
                            ans = rag.answer(q, chunks)
                            st.markdown("**ğŸ’¡ å›ç­”ï¼š**")
                            st.write(ans)

                            st.write("**ğŸ“š æ¥æºï¼š**")
                            for i, c in enumerate(chunks):
                                color = "ğŸŸ¢" if c["similarity"] < 0.2 else "ğŸŸ¡"
                                with st.expander(f"{color} æ¥æº {i+1}: {c['filename']} (è·ç¦»: {c['similarity']:.4f})"):
                                    st.write(c["content"][:500] + "â€¦")
                            st.session_state.last_query_time = datetime.now()
                    else:
                        st.warning("âš ï¸ æœªæ‰¾åˆ°ç›¸å…³å†…å®¹")
            else:
                st.error("âŒ è¯·è¾“å…¥é—®é¢˜")

if __name__ == "__main__":
    main()
